# AI Coding Agent Instructions ‚Äì Monitor de Precios SIPC

## Project Overview

**Academic Big Data Project** - Universidad Cat√≥lica del Uruguay (Campus Salto)  
**Course:** Big Data

This project builds a **Data Lake + ETL pipeline** orchestrated with **Apache Airflow** to analyze consumer price data from Uruguay's SIPC (Sistema de Informaci√≥n de Precios al Consumidor). The objective is to create a dimensional model (star schema) and calculate key economic indicators including price dispersion indexes, basket costs by retailer, and temporal price variations.

### Project Goals

1. **ETL Pipeline**: Ingest, transform, and model SIPC data using PySpark
2. **Dimensional Model**: Implement star schema with fact and dimension tables
3. **Business Metrics**: Calculate 6 key indicators for price analysis
4. **Orchestration**: Automate workflow with Apache Airflow
5. **Jupyter Notebooks**: Complete exploratory analysis and visualizations for metrics

## Architecture & Key Concepts

### Data Lake Zones (local filesystem under `data_sipc/`)

- `landing/` ‚Äì Raw CSV files from Cat√°logo de Datos Abiertos (NOT versioned in git)
- `raw/` ‚Äì Cleaned and typed data (PySpark processing)
- `refined/` ‚Äì Analytical model (star schema: dimensions + fact tables)
- `exports_dashboard/` ‚Äì Final datasets for Jupyter dashboard

### Tech Stack

- **PySpark (local mode)**: All ETL transformations run on single-node Spark (`spark.master("local[*]")`)
- **Apache Airflow**: Orchestration via Docker container, uses SequentialExecutor + SQLite
- **Docker Compose**: Two services ‚Äì `airflow` and `jupyter` (pyspark-notebook)
- **Parquet**: Columnar storage format for all processed data
- **No distributed cluster**: Intentional per project requirements (local mode only)

### Data Sources

- **Primary**: SIPC CSV files from https://catalogodatos.gub.uy/
  - `precios.csv`: ~20M+ price observations (fecha, producto_id, establecimiento_id, precio)
  - `productos.csv`: ~379 product catalog entries
  - `establecimientos.csv`: ~852 retail locations
- **Secondary**: CBAEN 2024 basket definition (docs/canasta_basica_cbaen_2024.md)

## Development Workflows

### Starting the Environment

```bash
# Start Airflow + Jupyter
docker-compose up -d

# Access points:
# - Airflow UI: http://localhost:8080 (no auth)
# - Jupyter Lab: http://localhost:8888
```

### DAG Development Pattern

1. Airflow DAGs live in `airflow/dags/monitor_precios_dag.py`
2. DAG imports Python modules from `src/` (mounted as volume in container)
3. Data processing logic goes in `src/transform/` or `src/ingestion/`, NOT in DAG file
4. DAG file should only define task orchestration (PythonOperator calls)

### Spark Session Pattern

- Reusable Spark session factory in `src/utils/spark_session.py`
- Always use `spark.master("local[*]")` ‚Äì no cluster deployment
- Common config: SQLite support for Airflow metadata, Parquet optimization

### Path Management

- Centralize all data paths in `src/utils/paths.py`
- Use absolute paths when running inside Airflow container (`/opt/airflow/data_sipc`)
- Use relative paths when running notebooks (`./data_sipc` or `../data_sipc`)

## ETL Pipeline Structure

### Current Implementation Status ‚úÖ

```
‚úÖ ingestion/ingest_landing.py
  ‚Üì (validates and copies CSV files to landing/)
‚úÖ transform/build_raw.py
  ‚Üì (clean data types, handle nulls ‚Üí raw/ as Parquet)
  ‚Üì Processes 20M+ price records, 379 products, 852 establishments
‚úÖ transform/build_dimensions.py
  ‚Üì (creates dim_tiempo, dim_producto, dim_establecimiento, dim_ubicacion)
‚úÖ transform/build_facts.py
  ‚Üì (creates fact_precios with FK references to dimensions)
‚úÖ metrics/simple_metrics.py
  ‚Üì (calculates 6 core metrics ‚Üí exports_dashboard/)
```

**Pipeline Execution Time**: ~6 minutes end-to-end on local machine

### Data Model (Star Schema in `refined/`)

**Implemented Dimensions:**
- ‚úÖ `dim_tiempo`: fecha_id (PK), fecha, anio, mes, dia, trimestre, dia_semana, semana_anio, nombre_mes, nombre_dia
- ‚úÖ `dim_producto`: producto_id (PK), nombre_completo, nombre, categoria, subcategoria, marca, especificacion
- ‚úÖ `dim_establecimiento`: establecimiento_id (PK), nombre, razon_social, cadena, cadena_normalizada
- ‚úÖ `dim_ubicacion`: ubicacion_id (PK), establecimiento_id, departamento, ciudad, direccion, barrio

**Implemented Fact Table:**
- ‚úÖ `fact_precios`: fecha_id (FK), producto_id (FK), establecimiento_id (FK), ubicacion_id (FK), precio, oferta

File format: Parquet (non-partitioned due to permissions issues with massive partitioning)

## Project-Specific Conventions

### Module Organization

- `src/ingestion/`: Data acquisition and landing zone preparation
- `src/transform/`: PySpark ETL jobs (raw ‚Üí refined transformations)
- `src/metrics/`: Business logic for KPIs (dispersion index, canasta b√°sica)
- `src/utils/`: Shared utilities (Spark session, path helpers)

### Naming Patterns

- DAG file: `monitor_precios_dag.py` (singular, descriptive)
- Transform scripts: `build_*.py` (verb prefix: build, create, compute)
- Dimension tables: `dim_*` (e.g., `dim_tiempo`, `dim_producto`)
- Fact tables: `fact_*` (e.g., `fact_precios`)

### Airflow Container Mounts

```yaml
volumes:
  - ./airflow/dags:/opt/airflow/dags
  - ./data_sipc:/opt/airflow/data_sipc # ‚Üê data lake access
  - ./src:/opt/airflow/src # ‚Üê Python modules
```

‚ö†Ô∏è Code changes in `src/` are immediately available to DAGs (no rebuild needed)

### Jupyter Notebooks

- `01_exploracion.ipynb`: Initial data exploration (understand SIPC CSV structure)
- `02_modelo_datos.ipynb`: Design and validate star schema model
- `03_dashboard.ipynb`: Final visualizations and metrics dashboard

Notebooks import from `../src/` and access `../data_sipc/` (parent directory)

## Dependencies & Installation

### Container Images

- Airflow: `apache/airflow:2.9.2` + pyspark, pandas
- Jupyter: `jupyter/pyspark-notebook:latest` (includes Spark pre-configured)

### Python Packages (see `requirements.txt`)

- `pyspark`: DataFrame API and SQL engine
- `pandas`: Data manipulation for dashboard exports
- `pyarrow`: Fast Parquet I/O
- `matplotlib`: Visualization in notebooks

## Important Constraints

1. **No external databases**: All storage is filesystem-based (Parquet files)
2. **No authentication**: Airflow webserver auth disabled for simplicity
3. **Local execution only**: Spark runs in `local[*]` mode, not on YARN/K8s
4. **SQLite for Airflow metadata**: Not PostgreSQL (acceptable for academic project)

## Common Tasks

### Add a new transformation step

1. Create module in `src/transform/my_transform.py`
2. Implement function that takes SparkSession, reads from `raw/`, writes to `refined/`
3. Add PythonOperator task in `monitor_precios_dag.py`
4. Define task dependencies with `>>` operator

### Debug ETL failures

1. Check Airflow logs: `./airflow/logs/<dag_id>/<task_id>/<timestamp>/`
2. Run transformation script directly in Jupyter to test
3. Validate Parquet schema with `spark.read.parquet(...).printSchema()`

### Add new metrics

1. Create module in `src/metrics/` (e.g., `dispersion_index.py`)
2. Read from `refined/` fact and dimension tables
3. Aggregate using PySpark SQL or DataFrame API
4. Export results to `exports_dashboard/` as CSV or Parquet

## Business Metrics (Required for Dashboard)

### Implemented Metrics ‚úÖ

All 6 required metrics are implemented in `src/metrics/simple_metrics.py` and exported to `exports_dashboard/`:

1. ‚úÖ **Precio promedio por producto**: Average price grouped by product/year/month
   - Output: `precio_promedio.parquet`
2. ‚úÖ **Variaci√≥n porcentual mensual**: Price change % comparing current vs. previous month
   - Output: `variacion_mensual.parquet`
3. ‚úÖ **Precio m√≠nimo y m√°ximo**: Min/max prices per product/period
   - Output: `min_max_precios.parquet`
4. ‚úÖ **Costo de canasta b√°sica por supermercado**: Based on CBAEN 2024 basket (80+ products)
   - Output: `canasta_basica.parquet`
5. ‚úÖ **√çndice de dispersi√≥n de precios**: `(precio_max - precio_min) / precio_promedio`
   - Output: `dispersion_precios.parquet`
6. ‚úÖ **Ranking de supermercados**: Stores ranked by total basket cost
   - Output: `ranking_supermercados.parquet`

### Known Technical Solutions

**Permission Handling in Spark Writes:**
- Issue: `mode("overwrite")` fails with "Mkdirs failed" errors due to Docker volume permissions
- Solution: Manual directory cleanup via `_clean_output_dir()` function before writing
- Implementation: Added to `build_raw.py`, `build_dimensions.py`, `build_facts.py`, `simple_metrics.py`
- Pattern: Use `shutil.rmtree()` + Hadoop FileSystem cleanup, then write without `mode("overwrite")`

## References

- SIPC Open Data: https://catalogodatos.gub.uy/
- Star Schema pattern: Kimball methodology (fact table + dimension tables)
- Airflow best practices: Keep DAGs declarative, logic in Python modules

---

## Work Plan and Pending Tasks

### ‚úÖ Completed (as of December 1, 2024)

**Infrastructure & Setup:**
- ‚úÖ Docker Compose environment with Airflow + Jupyter
- ‚úÖ Data Lake directory structure (landing, raw, refined, exports_dashboard)
- ‚úÖ Spark session utilities and path management
- ‚úÖ CSV data ingestion (20M+ records, 379 products, 852 establishments)

**ETL Pipeline:**
- ‚úÖ `build_raw.py`: Clean and type CSV data ‚Üí Parquet (raw zone)
- ‚úÖ `build_dimensions.py`: Create 4 dimensional tables
- ‚úÖ `build_facts.py`: Build fact table with FK relationships
- ‚úÖ `simple_metrics.py`: Calculate all 6 required business metrics
- ‚úÖ Airflow DAG orchestration (end-to-end pipeline working)

**Jupyter Notebooks:**
- ‚úÖ `01_exploracion.ipynb`: Complete exploratory data analysis
- ‚úÖ `02_modelo_datos.ipynb`: Star schema documentation and validation
- ‚úÖ `03_dashboard.ipynb`: All 6 metrics with visualizations, analysis, and conclusions

**Technical Fixes:**
- ‚úÖ Resolved Spark write permission issues in Docker volumes
- ‚úÖ CSV delimiter and encoding corrections
- ‚úÖ Removed partitioning strategy to avoid filesystem conflicts

### üîÑ Current Focus (December 1-2, 2024)

**DEADLINE: December 2, 2024**

**Priority Tasks:**
- [x] Complete all 3 Jupyter notebooks with 6 metrics
- [x] Ensure pipeline runs successfully end-to-end
- [ ] Test notebooks execution from clean state
- [ ] Final validation of all metrics calculations
- [ ] Code cleanup and commenting

**Optional Enhancements (if time permits):**
- [ ] Add data quality checks to Airflow DAG
- [ ] Improve README with usage examples
- [ ] Add docstrings to key functions
- [ ] Create simple validation tests

**Out of Scope for Current Iteration:**
- Technical report (not required for Dec 2 deadline)
- Presentation slides (not required for Dec 2 deadline)
- Advanced visualizations beyond current notebooks
- Extensive documentation beyond README

### üéØ Deliverables Checklist

**Required for December 2, 2024:**
- [x] **Source Code**: Complete ETL pipeline + notebooks
- [x] **ETL Pipeline**: Functioning Airflow DAG with all transformations
- [x] **Star Schema**: Dimensional model fully implemented
- [x] **6 Metrics**: All business indicators calculated and exported
- [x] **3 Notebooks**: Exploration, model documentation, and dashboard
- [ ] **Pipeline Testing**: Verify end-to-end execution
- [ ] **README**: Usage instructions for running the project

**Quality Standards (Current Iteration):**
- [x] Pipeline runs successfully end-to-end without manual intervention
- [x] All 6 metrics generate correct outputs
- [x] Notebooks execute without errors
- [ ] Basic code documentation (comments on complex logic)
- [ ] README with clear setup and usage instructions

### üìä Project Metrics & Success Criteria

**Technical Success Criteria:**
- ‚úÖ ETL pipeline processes 20M+ records in <10 minutes
- ‚úÖ Star schema correctly models dimensional relationships
**Technical Success Criteria:**
- ‚úÖ ETL pipeline processes 20M+ records in <10 minutes
- ‚úÖ Star schema correctly models dimensional relationships
- ‚úÖ All 6 business metrics calculate without errors
- ‚úÖ Airflow orchestration executes all tasks successfully
- ‚úÖ Notebooks generate visualizations without errors
- [ ] Pipeline can be executed from clean state
- [ ] All components documented in README
**Identified Risks:**
1. **Data Quality Issues**: Missing values, inconsistent formats
   - Mitigation: Comprehensive validation in raw zone transformation
2. **Performance Bottlenecks**: Large dataset processing time
   - Mitigation: Already optimized (6-min runtime), use Parquet compression
3. **Technical Complexity**: Airflow/Spark integration challenges
   - Mitigation: Working pipeline achieved, document all solutions
### üìÖ Timeline for Dec 2 Deadline

**December 1 (Today):**
- ‚úÖ Complete all 3 notebooks with 6 metrics
- ‚úÖ Verify pipeline execution
- [ ] Test from clean environment
- [ ] Update README with usage instructions

**December 2 (Deadline):**
- [ ] Final testing and validation
- [ ] Code cleanup
- [ ] Submission preparation
# Stop environment
docker-compose down

# Rebuild after code changes (if needed)
docker-compose up -d --build

# View Airflow logs
docker logs airflow --tail 100

# Access Airflow container
docker exec -it airflow bash

# Clean data directories (reset pipeline)
docker exec airflow rm -rf /opt/airflow/data_sipc/raw/* /opt/airflow/data_sipc/refined/* /opt/airflow/data_sipc/exports_dashboard/*

# Fix permissions (if needed)
docker exec airflow chmod -R 777 /opt/airflow/data_sipc/

# Trigger DAG manually
docker exec airflow airflow dags trigger monitor_precios_sipc_etl

# Check DAG status
docker exec airflow airflow dags list-runs -d monitor_precios_sipc_etl
```
