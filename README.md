# Monitor de Precios SIPC â€“ Obligatorio Big Data

Proyecto del curso **Big Data** (UCU â€“ Salto) que construye un **Data Lake** y un **pipeline ETL orquestado con Apache Airflow** para analizar los datos de precios del **SIPC** (Sistema de InformaciÃ³n de Precios al Consumidor).

## ğŸ¯ Objetivos

- Limpiar y unificar las tablas de precios, productos y establecimientos del SIPC
- Construir un **modelo tipo estrella** (dimensiones + hechos de precios)
- Calcular 6 mÃ©tricas clave sobre evoluciÃ³n de precios y canasta bÃ¡sica
- Exponer resultados en **dashboard Jupyter**

## ğŸ§© MÃ©tricas Implementadas

El proyecto calcula las siguientes **6 mÃ©tricas principales**:

1. **Precio promedio por producto**  
   Promedio del precio de un producto en un perÃ­odo y nivel de agregaciÃ³n determinado (por establecimiento, por cadena, por zona, etc.).  
   Ejemplo: precio promedio mensual del â€œArroz 1 kgâ€ por supermercado.

2. **VariaciÃ³n porcentual diaria/mensual**  
   Mide cuÃ¡nto variÃ³ el precio con respecto al perÃ­odo anterior.  
   FÃ³rmula genÃ©rica:  
   \[
   \text{VarPct} = \frac{\text{Precio actual} - \text{Precio anterior}}{\text{Precio anterior}}
   \]  
   Se calcula tanto **dÃ­a a dÃ­a** como **mes a mes** para productos y/o canasta.

3. **Precio mÃ­nimo y mÃ¡ximo**  
   Para cada producto y perÃ­odo, se calcula el **precio mÃ­nimo** y **mÃ¡ximo** observado entre todos los establecimientos.  
   Permite identificar supermercados mÃ¡s caros/baratos para cada producto.

4. **Costo de canasta bÃ¡sica por supermercado**  
   Se define una **canasta bÃ¡sica** como conjunto de productos seleccionados.  
   Para cada supermercado y perÃ­odo (por ejemplo, mes), se suma el precio de esos productos â†’ **costo total de la canasta**.  
   Permite comparar el â€œcosto de llenar el carritoâ€ entre supermercados.

5. **Ãndice de dispersiÃ³n de precios**  
   Mide cuÃ¡n dispersos estÃ¡n los precios de un producto entre establecimientos.  
   FÃ³rmula propuesta:  
   \[
   \text{Ãndice de dispersiÃ³n} = \frac{\text{Precio mÃ¡ximo} - \text{Precio mÃ­nimo}}{\text{Precio promedio}}
   \]  
   Valores altos indican gran diferencia de precios entre comercios.

6. **Ranking de supermercados segÃºn costo total**  
   A partir del costo de la canasta bÃ¡sica, se genera un ranking de supermercados (del mÃ¡s barato al mÃ¡s caro) para un perÃ­odo dado.  
   Puede filtrarse por ciudad/zona, cadena, etc.

Estas mÃ©tricas se calculan sobre la **capa Refined** del Data Lake y se utilizan en el dashboard final.

## ğŸ—ï¸ Arquitectura

### Data Lake (filesystem local)

```
data_sipc/
â”œâ”€â”€ landing/          # CSV originales del SIPC (no versionados)
â”œâ”€â”€ raw/              # Parquet limpio y tipado (PySpark)
â”œâ”€â”€ refined/          # Modelo estrella (dimensiones + hechos)
â””â”€â”€ exports_dashboard/ # Datasets finales para visualizaciÃ³n
```

### Stack TecnolÃ³gico

- **PySpark** (modo `local[*]`) â€“ Transformaciones ETL sin cluster distribuido
- **Apache Airflow 2.9.2** â€“ OrquestaciÃ³n (SequentialExecutor + SQLite)
- **Docker Compose** â€“ Contenedores `airflow` y `jupyter`
- **Parquet** â€“ Formato de almacenamiento optimizado

## ğŸš€ Inicio RÃ¡pido

### Prerequisitos

- Docker y Docker Compose instalados
- 4GB RAM disponible

### Levantar el entorno

```bash
# Clonar repositorio
git clone https://github.com/clara-s-n/monitor-precios-sipc.git
cd monitor-precios-sipc

# Iniciar servicios
docker-compose up -d

# Verificar estado
docker-compose ps
```

### Acceso a interfaces

| Servicio    | URL                   | Credenciales      |
| ----------- | --------------------- | ----------------- |
| Airflow UI  | http://localhost:8080 | Sin autenticaciÃ³n |
| Jupyter Lab | http://localhost:8888 | Token en logs     |

```bash
# Obtener token de Jupyter
docker logs jupyter-spark | grep "token="
```

### Ejecutar pipeline ETL

1. Colocar archivos CSV del SIPC en `data_sipc/landing/`:

   - `precios.csv`
   - `productos.csv`
   - `establecimientos.csv`

2. En Airflow UI (http://localhost:8080), activar DAG `monitor_precios_sipc_etl`

3. Monitorear ejecuciÃ³n en el panel de tareas

## ğŸ“‚ Estructura del Proyecto

```
monitor-precios-sipc/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile              # Imagen custom con PySpark
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ monitor_precios_dag.py  # âœ… OrquestaciÃ³n ETL principal
â”‚   â””â”€â”€ logs/                   # Logs de ejecuciÃ³n
â”‚
â”œâ”€â”€ src/                        # LÃ³gica de negocio (montado en containers)
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ ingest_landing.py   # âœ… ValidaciÃ³n y copia de CSVs
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”œâ”€â”€ build_raw.py        # âœ… Landing â†’ Raw (Parquet)
â”‚   â”‚   â”œâ”€â”€ build_dimensions.py # ConstrucciÃ³n de dimensiones
â”‚   â”‚   â””â”€â”€ build_facts.py      # Tabla de hechos
â”‚   â”œâ”€â”€ metrics/                # CÃ¡lculo de KPIs
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ spark_session.py    # âœ… Factory de sesiones Spark
â”‚       â””â”€â”€ paths.py            # âœ… GestiÃ³n de rutas del data lake
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploracion.ipynb    # AnÃ¡lisis exploratorio
â”‚   â”œâ”€â”€ 02_modelo_datos.ipynb   # DiseÃ±o star schema
â”‚   â””â”€â”€ 03_dashboard.ipynb      # Visualizaciones finales
â”‚
â”œâ”€â”€ data_sipc/                  # Data Lake (gitignored)
â””â”€â”€ docker-compose.yaml
```

âœ… = Implementado | ğŸ”² = Pendiente

## ğŸ“Š Pipeline ETL

### Flujo de datos

```
ğŸ“¥ Landing Zone (CSV)
    â†“ ingest_landing.py (validaciÃ³n + copia)

ğŸ§¹ Raw Zone (Parquet limpio)
    â†“ build_raw.py (limpieza + tipado)

ğŸ“ Refined Zone (Star Schema)
    â†“ build_dimensions.py
    â”‚   â†’ dim_tiempo, dim_producto, dim_establecimiento, dim_ubicacion
    â†“ build_facts.py
    â”‚   â†’ fact_precios

ğŸ“ˆ Exports Dashboard
    â†“ metrics/* (KPIs)
    â”‚   â†’ precio_promedio, dispersion_index, canasta_basica, ranking
```

### Modelo de Datos (Star Schema)

**Dimensiones:**

- `dim_tiempo`: fecha, aÃ±o, mes, dÃ­a, trimestre
- `dim_producto`: producto_id, nombre, categorÃ­a, subcategorÃ­a, marca
- `dim_establecimiento`: establecimiento_id, nombre, cadena
- `dim_ubicacion`: ubicacion_id, departamento, ciudad, direcciÃ³n

**Hechos:**

- `fact_precios`: precio, fecha_id, producto_id, establecimiento_id, ubicacion_id, unidad

## ğŸ”§ Desarrollo

### Estructura del cÃ³digo ETL

El pipeline ETL estÃ¡ organizado en mÃ³dulos reutilizables en `src/`:

**Ingesta (`src/ingestion/`):**
- `ingest_landing.py`: Valida y copia archivos CSV a la landing zone
  - Verifica estructura de columnas esperadas
  - Maneja encoding ISO-8859-1 y delimitador `;`
  - Genera metadata de ingesta

**Transformaciones (`src/transform/`):**
- `build_raw.py`: Procesa CSVs a Parquet con limpieza y tipado
  - Convierte fechas, normaliza columnas
  - Filtra registros invÃ¡lidos
  - Particiona por fecha para optimizar consultas

- `build_dimensions.py`: Construye las 4 dimensiones del modelo estrella
  - `dim_tiempo`: Atributos temporales derivados de fechas
  - `dim_producto`: CatÃ¡logo completo de productos
  - `dim_establecimiento`: InformaciÃ³n de comercios
  - `dim_ubicacion`: Datos geogrÃ¡ficos

- `build_facts.py`: Crea tabla de hechos con claves forÃ¡neas
  - Joins con todas las dimensiones
  - Mantiene medidas (precio, unidad)
  - Particionado por fecha

**MÃ©tricas (`src/metrics/`):**
- `simple_metrics.py`: Calcula las 6 mÃ©tricas de negocio
  1. Precio promedio por producto y perÃ­odo
  2. Precios mÃ­nimos y mÃ¡ximos
  3. Ãndice de dispersiÃ³n de precios
  4. Costo de canasta bÃ¡sica por supermercado
  5. Ranking de supermercados por costo
  6. VariaciÃ³n porcentual mensual

**Utilidades (`src/utils/`):**
- `spark_session.py`: Factory de sesiones Spark (modo local)
- `paths.py`: GestiÃ³n centralizada de rutas del Data Lake

### Editar transformaciones ETL

Los mÃ³dulos en `src/` estÃ¡n montados como volumen en el contenedor de Airflow, por lo que los cambios se reflejan inmediatamente sin necesidad de reconstruir la imagen.

```bash
# Editar archivo
vim src/transform/build_dimensions.py

# Probar localmente con PySpark
cd /ruta/proyecto
python -c "from src.transform.build_dimensions import build_dimensions; build_dimensions()"

# O ejecutar desde Airflow UI
# (activa manualmente el DAG monitor_precios_sipc_etl)
```

### Ejecutar pipeline completo

```bash
# Asegurar que los CSV estÃ¡n en landing/
ls -la data_sipc/landing/*.csv

# Desde Airflow UI:
# 1. Ir a http://localhost:8080
# 2. Buscar DAG 'monitor_precios_sipc_etl'
# 3. Activar toggle a ON
# 4. Trigger DAG manualmente con botÃ³n â–¶ï¸

# Verificar outputs
ls -la data_sipc/raw/
ls -la data_sipc/refined/
ls -la data_sipc/exports_dashboard/
```

### Estructura de datos generada

**Raw zone** (`data_sipc/raw/`):
- `precios/`: Precios limpios particionados por fecha
- `productos/`: CatÃ¡logo de productos
- `establecimientos/`: InformaciÃ³n de comercios

**Refined zone** (`data_sipc/refined/`):
- `dim_tiempo/`: DimensiÃ³n temporal
- `dim_producto/`: DimensiÃ³n de productos
- `dim_establecimiento/`: DimensiÃ³n de establecimientos
- `dim_ubicacion/`: DimensiÃ³n geogrÃ¡fica
- `fact_precios/`: Tabla de hechos (particionada por fecha)

**Exports** (`data_sipc/exports_dashboard/`):
- `precio_promedio.parquet`
- `min_max_precios.parquet`
- `dispersion_precios.parquet`
- `canasta_basica.parquet`
- `ranking_supermercados.parquet`
- `variacion_mensual.parquet`

### Leer resultados en notebooks

Ver la **guÃ­a completa de notebooks** en [`notebooks/README.md`](notebooks/README.md).

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Dashboard").getOrCreate()

# Leer mÃ©tricas
precio_prom = spark.read.parquet("../data_sipc/exports_dashboard/precio_promedio.parquet")
precio_prom.show()

# AnÃ¡lisis con Pandas
df_pandas = precio_prom.toPandas()
```

## ğŸ““ Jupyter Notebooks

El proyecto incluye 3 notebooks interactivos para anÃ¡lisis y visualizaciÃ³n:

### 1. `01_exploracion.ipynb` - AnÃ¡lisis Exploratorio

**PropÃ³sito:** Entender la estructura y calidad de los datos raw

**Contenido:**
- Carga de datos desde zona RAW (Parquet)
- EstadÃ­sticas descriptivas (20M+ registros de precios)
- Distribuciones temporales y geogrÃ¡ficas
- AnÃ¡lisis de calidad (nulos, duplicados, outliers)
- VerificaciÃ³n de integridad referencial
- Visualizaciones de precios por categorÃ­a

**DuraciÃ³n:** ~10-15 minutos

### 2. `02_modelo_datos.ipynb` - Star Schema

**PropÃ³sito:** Documentar y validar el modelo dimensional

**Contenido:**
- ExplicaciÃ³n del Star Schema implementado
- DescripciÃ³n de las 4 dimensiones
- Tabla de hechos con 20M+ observaciones
- ValidaciÃ³n de integridad referencial (100%)
- Ejemplos de consultas analÃ­ticas multidimensionales
- Benchmark de performance

**DuraciÃ³n:** ~15-20 minutos

### 3. `03_dashboard.ipynb` - Dashboard de MÃ©tricas

**PropÃ³sito:** Visualizar y analizar las 6 mÃ©tricas principales

**Contenido:**
- **MÃ©trica 1:** Precio promedio - EvoluciÃ³n temporal
- **MÃ©trica 2:** VariaciÃ³n mensual - Histogramas y tendencias
- **MÃ©trica 3:** Min/Max precios - Rangos por producto
- **MÃ©trica 4:** Canasta bÃ¡sica - ComparaciÃ³n por supermercado
- **MÃ©trica 5:** DispersiÃ³n - Variabilidad de precios
- **MÃ©trica 6:** Ranking - Supermercados ordenados por costo
- AnÃ¡lisis integrado con correlaciones
- Dashboard consolidado (4 paneles)
- Conclusiones y recomendaciones

**DuraciÃ³n:** ~20-30 minutos

### CÃ³mo Ejecutar los Notebooks

#### Paso 1: Ejecutar el Pipeline ETL

**Prerequisito:** Los notebooks requieren que el pipeline haya generado los datos primero.

```bash
# 1. Asegurar que los CSV estÃ¡n en landing/
ls -la data_sipc/landing/*.csv

# 2. Iniciar servicios
docker-compose up -d

# 3. Ejecutar DAG en Airflow
# Ir a http://localhost:8080
# Activar y ejecutar 'monitor_precios_sipc_etl'

# 4. Verificar que se generaron los datos
ls -la data_sipc/raw/
ls -la data_sipc/refined/
ls -la data_sipc/exports_dashboard/
```

#### Paso 2: Acceder a Jupyter Lab

```bash
# 1. Obtener el token de acceso
docker logs jupyter-spark 2>&1 | grep "token="

# 2. Abrir en navegador
# http://localhost:8888/?token=XXXXXXXXXX
```

#### Paso 3: Ejecutar Notebooks en Orden

```
ğŸ““ Orden recomendado:
   01_exploracion.ipynb  â†’  02_modelo_datos.ipynb  â†’  03_dashboard.ipynb
```

**Opciones de ejecuciÃ³n:**
- **Celda por celda:** `Shift + Enter`
- **Todo el notebook:** Menu â†’ Run â†’ Run All Cells
- **Hasta una celda:** Menu â†’ Run â†’ Run All Above Selected Cell

#### Verificar Datos Antes de Ejecutar

Los notebooks esperan encontrar datos en estas rutas (relativas desde `notebooks/`):

```python
# Raw zone (para 01_exploracion y 02_modelo_datos)
'../data_sipc/raw/precios.parquet'
'../data_sipc/raw/productos.parquet'
'../data_sipc/raw/establecimientos.parquet'

# Refined zone (para 02_modelo_datos)
'../data_sipc/refined/dim_tiempo.parquet'
'../data_sipc/refined/dim_producto.parquet'
'../data_sipc/refined/dim_establecimiento.parquet'
'../data_sipc/refined/dim_ubicacion.parquet'
'../data_sipc/refined/fact_precios.parquet'

# Exports (para 03_dashboard)
'../data_sipc/exports_dashboard/precio_promedio.parquet'
'../data_sipc/exports_dashboard/variacion_mensual.parquet'
'../data_sipc/exports_dashboard/min_max_precios.parquet'
'../data_sipc/exports_dashboard/canasta_basica.parquet'
'../data_sipc/exports_dashboard/dispersion_precios.parquet'
'../data_sipc/exports_dashboard/ranking_supermercados.parquet'
```

### SoluciÃ³n de Problemas Comunes

#### Error: "FileNotFoundError"

**Causa:** Pipeline ETL no ejecutado o datos incompletos

**SoluciÃ³n:**
```bash
# Ejecutar pipeline completo
docker exec airflow airflow dags trigger monitor_precios_sipc_etl

# Esperar finalizaciÃ³n (~6 minutos)
docker exec airflow airflow dags list-runs -d monitor_precios_sipc_etl
```

#### Error: "No module named 'pyspark'"

**Causa:** Ejecutando notebook fuera del contenedor

**SoluciÃ³n:**
- Usar Jupyter Lab dentro del contenedor: http://localhost:8888
- No ejecutar notebooks directamente en el host

#### Kernel muere o se queda sin memoria

**Causa:** Datasets grandes consumiendo mucha RAM

**SoluciÃ³n:**
- Reiniciar kernel: Menu â†’ Kernel â†’ Restart Kernel
- Ejecutar celdas en orden (no todas a la vez)
- Aumentar memoria del contenedor en `docker-compose.yaml`:
  ```yaml
  jupyter:
    deploy:
      resources:
        limits:
          memory: 6G  # Aumentar de 4G a 6G
  ```

#### Visualizaciones no se renderizan

**Causa:** ConfiguraciÃ³n de matplotlib

**SoluciÃ³n:**
```python
# AÃ±adir al inicio del notebook
%matplotlib inline
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (14, 6)
```

### CaracterÃ­sticas de los Notebooks

**Visualizaciones incluidas:**
- ğŸ“ˆ GrÃ¡ficos de lÃ­neas (evoluciÃ³n temporal)
- ğŸ“Š Histogramas (distribuciones)
- ğŸ“¦ Boxplots (comparaciones estadÃ­sticas)
- ğŸ¯ Scatter plots (correlaciones)
- ğŸ† GrÃ¡ficos de barras (rankings)
- ğŸ“‰ Dashboards multi-panel (vista consolidada)

**LibrerÃ­as utilizadas:**
- `pyspark` - Procesamiento de datos
- `pandas` - ManipulaciÃ³n para visualizaciÃ³n
- `matplotlib` - GrÃ¡ficos estÃ¡ticos
- `seaborn` - Visualizaciones estadÃ­sticas mejoradas

**Ventajas del enfoque:**
- âœ… SeparaciÃ³n de concerns: ETL en Airflow, anÃ¡lisis en Jupyter
- âœ… Datos pre-procesados para anÃ¡lisis rÃ¡pido
- âœ… Reproducibilidad: notebooks versionados en git
- âœ… Interactividad: exploraciÃ³n ad-hoc sin reejecutar pipeline
