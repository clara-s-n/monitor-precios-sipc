# Monitor de Precios SIPC â€“ Obligatorio Big Data

Proyecto del curso **Big Data** (UCU â€“ Salto) que construye un **Data Lake** y un **pipeline ETL orquestado con Apache Airflow** para analizar los datos de precios del **SIPC** (Sistema de InformaciÃ³n de Precios al Consumidor).

## ğŸ¯ Objetivos

- Limpiar y unificar las tablas de precios, productos y establecimientos del SIPC
- Construir un **modelo tipo estrella** (dimensiones + hechos de precios)
- Calcular 6 mÃ©tricas clave sobre evoluciÃ³n de precios y canasta bÃ¡sica
- Exponer resultados en **dashboard Jupyter**

## ğŸ§© MÃ©tricas Implementadas

El proyecto calcula las siguientes **6 mÃ©tricas principales**:

1. **Precio promedio por producto**  
   Promedio del precio de un producto en un perÃ­odo y nivel de agregaciÃ³n determinado (por establecimiento, por cadena, por zona, etc.).  
   Ejemplo: precio promedio mensual del â€œArroz 1 kgâ€ por supermercado.

2. **VariaciÃ³n porcentual diaria/mensual**  
   Mide cuÃ¡nto variÃ³ el precio con respecto al perÃ­odo anterior.  
   FÃ³rmula genÃ©rica:  
   \[
   \text{VarPct} = \frac{\text{Precio actual} - \text{Precio anterior}}{\text{Precio anterior}}
   \]  
   Se calcula tanto **dÃ­a a dÃ­a** como **mes a mes** para productos y/o canasta.

3. **Precio mÃ­nimo y mÃ¡ximo**  
   Para cada producto y perÃ­odo, se calcula el **precio mÃ­nimo** y **mÃ¡ximo** observado entre todos los establecimientos.  
   Permite identificar supermercados mÃ¡s caros/baratos para cada producto.

4. **Costo de canasta bÃ¡sica por supermercado**  
   Se define una **canasta bÃ¡sica** como conjunto de productos seleccionados.  
   Para cada supermercado y perÃ­odo (por ejemplo, mes), se suma el precio de esos productos â†’ **costo total de la canasta**.  
   Permite comparar el â€œcosto de llenar el carritoâ€ entre supermercados.

5. **Ãndice de dispersiÃ³n de precios**  
   Mide cuÃ¡n dispersos estÃ¡n los precios de un producto entre establecimientos.  
   FÃ³rmula propuesta:  
   \[
   \text{Ãndice de dispersiÃ³n} = \frac{\text{Precio mÃ¡ximo} - \text{Precio mÃ­nimo}}{\text{Precio promedio}}
   \]  
   Valores altos indican gran diferencia de precios entre comercios.

6. **Ranking de supermercados segÃºn costo total**  
   A partir del costo de la canasta bÃ¡sica, se genera un ranking de supermercados (del mÃ¡s barato al mÃ¡s caro) para un perÃ­odo dado.  
   Puede filtrarse por ciudad/zona, cadena, etc.

Estas mÃ©tricas se calculan sobre la **capa Refined** del Data Lake y se utilizan en el dashboard final.

## ğŸ—ï¸ Arquitectura

### Data Lake (filesystem local)

```
data_sipc/
â”œâ”€â”€ landing/          # CSV originales del SIPC (no versionados)
â”œâ”€â”€ raw/              # Parquet limpio y tipado (PySpark)
â”œâ”€â”€ refined/          # Modelo estrella (dimensiones + hechos)
â””â”€â”€ exports_dashboard/ # Datasets finales para visualizaciÃ³n
```

### Stack TecnolÃ³gico

- **PySpark** (modo `local[*]`) â€“ Transformaciones ETL sin cluster distribuido
- **Apache Airflow 2.9.2** â€“ OrquestaciÃ³n (SequentialExecutor + SQLite)
- **Docker Compose** â€“ Contenedores `airflow` y `jupyter`
- **Parquet** â€“ Formato de almacenamiento optimizado

## ğŸš€ Inicio RÃ¡pido

### Prerequisitos

- Docker y Docker Compose instalados
- 4GB RAM disponible

### Levantar el entorno

```bash
# Clonar repositorio
git clone https://github.com/clara-s-n/monitor-precios-sipc.git
cd monitor-precios-sipc

# Iniciar servicios
docker-compose up -d

# Verificar estado
docker-compose ps
```

### Acceso a interfaces

| Servicio    | URL                   | Credenciales      |
| ----------- | --------------------- | ----------------- |
| Airflow UI  | http://localhost:8080 | Sin autenticaciÃ³n |
| Jupyter Lab | http://localhost:8888 | Token en logs     |

```bash
# Obtener token de Jupyter
docker logs jupyter-spark | grep "token="
```

### Ejecutar pipeline ETL

1. Colocar archivos CSV del SIPC en `data_sipc/landing/`:

   - `precios.csv`
   - `productos.csv`
   - `establecimientos.csv`

2. En Airflow UI (http://localhost:8080), activar DAG `monitor_precios_sipc_etl`

3. Monitorear ejecuciÃ³n en el panel de tareas

## ğŸ“‚ Estructura del Proyecto

```
monitor-precios-sipc/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile              # Imagen custom con PySpark
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ monitor_precios_dag.py  # âœ… OrquestaciÃ³n ETL principal
â”‚   â””â”€â”€ logs/                   # Logs de ejecuciÃ³n
â”‚
â”œâ”€â”€ src/                        # LÃ³gica de negocio (montado en containers)
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ ingest_landing.py   # âœ… ValidaciÃ³n y copia de CSVs
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”œâ”€â”€ build_raw.py        # âœ… Landing â†’ Raw (Parquet)
â”‚   â”‚   â”œâ”€â”€ build_dimensions.py # ConstrucciÃ³n de dimensiones
â”‚   â”‚   â””â”€â”€ build_facts.py      # Tabla de hechos
â”‚   â”œâ”€â”€ metrics/                # CÃ¡lculo de KPIs
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ spark_session.py    # âœ… Factory de sesiones Spark
â”‚       â””â”€â”€ paths.py            # âœ… GestiÃ³n de rutas del data lake
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploracion.ipynb    # AnÃ¡lisis exploratorio
â”‚   â”œâ”€â”€ 02_modelo_datos.ipynb   # DiseÃ±o star schema
â”‚   â””â”€â”€ 03_dashboard.ipynb      # Visualizaciones finales
â”‚
â”œâ”€â”€ data_sipc/                  # Data Lake (gitignored)
â””â”€â”€ docker-compose.yaml
```

âœ… = Implementado | ğŸ”² = Pendiente

## ğŸ“Š Pipeline ETL

### Flujo de datos

```
ğŸ“¥ Landing Zone (CSV)
    â†“ ingest_landing.py (validaciÃ³n + copia)

ğŸ§¹ Raw Zone (Parquet limpio)
    â†“ build_raw.py (limpieza + tipado)

ğŸ“ Refined Zone (Star Schema)
    â†“ build_dimensions.py
    â”‚   â†’ dim_tiempo, dim_producto, dim_establecimiento, dim_ubicacion
    â†“ build_facts.py
    â”‚   â†’ fact_precios

ğŸ“ˆ Exports Dashboard
    â†“ metrics/* (KPIs)
    â”‚   â†’ precio_promedio, dispersion_index, canasta_basica, ranking
```

### Modelo de Datos (Star Schema)

**Dimensiones:**

- `dim_tiempo`: fecha, aÃ±o, mes, dÃ­a, trimestre
- `dim_producto`: producto_id, nombre, categorÃ­a, subcategorÃ­a, marca
- `dim_establecimiento`: establecimiento_id, nombre, cadena
- `dim_ubicacion`: ubicacion_id, departamento, ciudad, direcciÃ³n

**Hechos:**

- `fact_precios`: precio, fecha_id, producto_id, establecimiento_id, ubicacion_id, unidad

## ğŸ”§ Desarrollo

### Estructura del cÃ³digo ETL

El pipeline ETL estÃ¡ organizado en mÃ³dulos reutilizables en `src/`:

**Ingesta (`src/ingestion/`):**
- `ingest_landing.py`: Valida y copia archivos CSV a la landing zone
  - Verifica estructura de columnas esperadas
  - Maneja encoding ISO-8859-1 y delimitador `;`
  - Genera metadata de ingesta

**Transformaciones (`src/transform/`):**
- `build_raw.py`: Procesa CSVs a Parquet con limpieza y tipado
  - Convierte fechas, normaliza columnas
  - Filtra registros invÃ¡lidos
  - Particiona por fecha para optimizar consultas

- `build_dimensions.py`: Construye las 4 dimensiones del modelo estrella
  - `dim_tiempo`: Atributos temporales derivados de fechas
  - `dim_producto`: CatÃ¡logo completo de productos
  - `dim_establecimiento`: InformaciÃ³n de comercios
  - `dim_ubicacion`: Datos geogrÃ¡ficos

- `build_facts.py`: Crea tabla de hechos con claves forÃ¡neas
  - Joins con todas las dimensiones
  - Mantiene medidas (precio, unidad)
  - Particionado por fecha

**MÃ©tricas (`src/metrics/`):**
- `simple_metrics.py`: Calcula las 6 mÃ©tricas de negocio
  1. Precio promedio por producto y perÃ­odo
  2. Precios mÃ­nimos y mÃ¡ximos
  3. Ãndice de dispersiÃ³n de precios
  4. Costo de canasta bÃ¡sica por supermercado
  5. Ranking de supermercados por costo
  6. VariaciÃ³n porcentual mensual

**Utilidades (`src/utils/`):**
- `spark_session.py`: Factory de sesiones Spark (modo local)
- `paths.py`: GestiÃ³n centralizada de rutas del Data Lake

### Editar transformaciones ETL

Los mÃ³dulos en `src/` estÃ¡n montados como volumen en el contenedor de Airflow, por lo que los cambios se reflejan inmediatamente sin necesidad de reconstruir la imagen.

```bash
# Editar archivo
vim src/transform/build_dimensions.py

# Probar localmente con PySpark
cd /ruta/proyecto
python -c "from src.transform.build_dimensions import build_dimensions; build_dimensions()"

# O ejecutar desde Airflow UI
# (activa manualmente el DAG monitor_precios_sipc_etl)
```

### Ejecutar pipeline completo

```bash
# Asegurar que los CSV estÃ¡n en landing/
ls -la data_sipc/landing/*.csv

# Desde Airflow UI:
# 1. Ir a http://localhost:8080
# 2. Buscar DAG 'monitor_precios_sipc_etl'
# 3. Activar toggle a ON
# 4. Trigger DAG manualmente con botÃ³n â–¶ï¸

# Verificar outputs
ls -la data_sipc/raw/
ls -la data_sipc/refined/
ls -la data_sipc/exports_dashboard/
```

### Estructura de datos generada

**Raw zone** (`data_sipc/raw/`):
- `precios/`: Precios limpios particionados por fecha
- `productos/`: CatÃ¡logo de productos
- `establecimientos/`: InformaciÃ³n de comercios

**Refined zone** (`data_sipc/refined/`):
- `dim_tiempo/`: DimensiÃ³n temporal
- `dim_producto/`: DimensiÃ³n de productos
- `dim_establecimiento/`: DimensiÃ³n de establecimientos
- `dim_ubicacion/`: DimensiÃ³n geogrÃ¡fica
- `fact_precios/`: Tabla de hechos (particionada por fecha)

**Exports** (`data_sipc/exports_dashboard/`):
- `precio_promedio.parquet`
- `min_max_precios.parquet`
- `dispersion_precios.parquet`
- `canasta_basica.parquet`
- `ranking_supermercados.parquet`
- `variacion_mensual.parquet`

### Leer resultados en notebooks

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Dashboard").getOrCreate()

# Leer mÃ©tricas
precio_prom = spark.read.parquet("../data_sipc/exports_dashboard/precio_promedio.parquet")
precio_prom.show()

# AnÃ¡lisis con Pandas
df_pandas = precio_prom.toPandas()
```
