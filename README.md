# Monitor de Precios SIPC â€“ Obligatorio Big Data

Proyecto del curso **Big Data** (UCU â€“ Salto) que construye un **Data Lake** y un **pipeline ETL orquestado con Apache Airflow** para analizar los datos de precios del **SIPC** (Sistema de InformaciÃ³n de Precios al Consumidor).

El objetivo es:

- Limpiar y unificar las tablas de precios, productos y establecimientos.
- Construir un **modelo tipo estrella** con dimensiones (tiempo, producto, establecimiento, ubicaciÃ³n, etc.) y hechos de precios.
- Calcular mÃ©tricas de negocio sobre la evoluciÃ³n de precios y la canasta bÃ¡sica (ver secciÃ³n de mÃ©tricas).
- Exponer los resultados en un **dashboard en Jupyter**.

---

## ğŸ§© MÃ©tricas implementadas

El proyecto implementa las siguientes **6 mÃ©tricas principales**:

1. **Precio promedio por producto**  
   Promedio del precio de un producto en un perÃ­odo y nivel de agregaciÃ³n determinado (por establecimiento, por cadena, por zona, etc.).  
   Ejemplo: precio promedio mensual del â€œArroz 1 kgâ€ por supermercado.

2. **VariaciÃ³n porcentual diaria/mensual**  
   Mide cuÃ¡nto variÃ³ el precio con respecto al perÃ­odo anterior.  
   FÃ³rmula genÃ©rica:  
   \[
   \text{VarPct} = \frac{\text{Precio actual} - \text{Precio anterior}}{\text{Precio anterior}}
   \]  
   Se calcula tanto **dÃ­a a dÃ­a** como **mes a mes** para productos y/o canasta.

3. **Precio mÃ­nimo y mÃ¡ximo**  
   Para cada producto y perÃ­odo, se calcula el **precio mÃ­nimo** y **mÃ¡ximo** observado entre todos los establecimientos.  
   Permite identificar supermercados mÃ¡s caros/baratos para cada producto.

4. **Costo de canasta bÃ¡sica por supermercado**  
   Se define una **canasta bÃ¡sica** como conjunto de productos seleccionados.  
   Para cada supermercado y perÃ­odo (por ejemplo, mes), se suma el precio de esos productos â†’ **costo total de la canasta**.  
   Permite comparar el â€œcosto de llenar el carritoâ€ entre supermercados.

5. **Ãndice de dispersiÃ³n de precios**  
   Mide cuÃ¡n dispersos estÃ¡n los precios de un producto entre establecimientos.  
   FÃ³rmula propuesta:  
   \[
   \text{Ãndice de dispersiÃ³n} = \frac{\text{Precio mÃ¡ximo} - \text{Precio mÃ­nimo}}{\text{Precio promedio}}
   \]  
   Valores altos indican gran diferencia de precios entre comercios.

6. **Ranking de supermercados segÃºn costo total**  
   A partir del costo de la canasta bÃ¡sica, se genera un ranking de supermercados (del mÃ¡s barato al mÃ¡s caro) para un perÃ­odo dado.  
   Puede filtrarse por ciudad/zona, cadena, etc.

Estas mÃ©tricas se calculan sobre la **capa Refined** del Data Lake y se utilizan en el dashboard final.

---

## Arquitectura y tecnologÃ­as

- **Data Lake (local)** con zonas:
  - `landing/` â€“ CSV originales descargados desde el CatÃ¡logo de Datos Abiertos de Uruguay.
  - `raw/` â€“ datos limpios y tipados.
  - `refined/` â€“ modelo analÃ­tico (dimensiones + hechos).
- **OrquestaciÃ³n:** [Apache Airflow](https://airflow.apache.org/) (modo local, con SQLite).
- **Procesamiento de datos:**
  - **PySpark** (modo `local`) para las transformaciones principales.
  - **Python + Pandas** para exploraciÃ³n y generaciÃ³n de datasets finales para el dashboard.
- **VisualizaciÃ³n:** JupyterLab / Notebooks.
- **Contenedores:** Docker + `docker-compose`.

No se levanta un cluster distribuido real; se usa **Spark en modo local**, tal como permite el obligatorio.

---

## Estructura del repositorio

```text
monitor-precios-sipc/
â”œâ”€ README.md
â”œâ”€ .gitignore
â”œâ”€ requirements.txt
â”œâ”€ docker-compose.yml
â”‚
â”œâ”€ airflow/
â”‚  â”œâ”€ Dockerfile
â”‚  â”œâ”€ dags/
â”‚  â”‚  â””â”€ monitor_precios_dag.py
â”‚  â”œâ”€ logs/              # se generan en runtime
â”‚  â””â”€ plugins/
â”‚
â”œâ”€ src/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ config.py          # rutas, parÃ¡metros generales del proyecto
â”‚  â”œâ”€ utils/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â”œâ”€ spark_session.py
â”‚  â”‚  â””â”€ paths.py
â”‚  â”œâ”€ ingestion/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â”œâ”€ ingest_landing.py      # copiado/preparaciÃ³n de datos en landing
â”‚  â”œâ”€ transform/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â”œâ”€ build_raw.py
â”‚  â”‚  â”œâ”€ build_dimensions.py
â”‚  â”‚  â””â”€ build_facts.py
â”‚  â””â”€ metrics/
â”‚     â”œâ”€ __init__.py
â”‚     â”œâ”€ dispersion_index.py
â”‚     â””â”€ canasta_basica.py
â”‚
â”œâ”€ notebooks/
â”‚  â”œâ”€ 01_exploracion.ipynb
â”‚  â”œâ”€ 02_modelo_datos.ipynb
â”‚  â””â”€ 03_dashboard.ipynb
â”‚
â”œâ”€ data_sipc/
â”‚  â”œâ”€ landing/           # CSV originales del SIPC (NO versionados)
â”‚  â”œâ”€ raw/               # datos limpios / tipados
â”‚  â”œâ”€ refined/           # modelo analÃ­tico (dimensiones + hechos)
â”‚  â””â”€ exports_dashboard/ # datasets finales para dashboard
â”‚
â””â”€ docs/
   â”œâ”€ informe_obligatorio.md
   â”œâ”€ modelo_datos.md
   â”œâ”€ arquitectura_datalake.md
   â”œâ”€ decisiones_tecnicas.md
   â””â”€ metadata/
      â”œâ”€ metadatos-precios.csv
      â”œâ”€ metadatos-establecimientos.csv
      â””â”€ metadatos-productos.csv

```

âœ… Requisitos previos
OpciÃ³n A â€“ Con Docker (recomendada para Airflow + Jupyter)

Docker
y docker compose instalados.

OpciÃ³n B â€“ Sin Docker (entorno local con venv)

Python 3.10+ (ideal 3.11).

pip instalado.

Para usar PySpark local:

Basta con instalar el paquete pyspark (incluye Spark embebido).

No es obligatorio instalar Spark aparte ni configurar un cluster.

ğŸ”§ ConfiguraciÃ³n del entorno local (venv)

Si querÃ©s correr partes del proyecto sin Docker (scripts o notebooks en tu mÃ¡quina), podÃ©s usar un entorno virtual.

1. Crear el entorno virtual

Desde la raÃ­z del proyecto:

Windows (PowerShell)
python -m venv .venv

Si python no apunta a 3.10/3.11, podÃ©s usar:

py -3.11 -m venv .venv

Linux / macOS
python3 -m venv .venv

2. Activar el entorno virtual
   Windows (PowerShell)
   .\.venv\Scripts\Activate.ps1

Si aparece un error de ejecuciÃ³n de scripts, puede ser necesario ajustar la ExecutionPolicy una vez:

Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

Linux / macOS
source .venv/bin/activate

Vas a ver el prefijo (.venv) en tu terminal.

3. Instalar dependencias

Con el venv activado:

pip install --upgrade pip
pip install -r requirements.txt

requirements.txt deberÃ­a incluir al menos:

pyspark
pandas
pyarrow
matplotlib
jupyter

(Se pueden agregar mÃ¡s librerÃ­as segÃºn las vayan usando.)

4. Ejecutar scripts del proyecto (modo local)

Ejemplos (con venv activado):

python -m src.transform.build_raw
python -m src.transform.build_dimensions
python -m src.transform.build_facts
python -m src.metrics.dispersion_index
python -m src.metrics.canasta_basica

Cada script leerÃ¡ desde data_sipc/landing o data_sipc/raw y escribirÃ¡ en data_sipc/raw / data_sipc/refined segÃºn la configuraciÃ³n en src/config.py.

5. Usar Jupyter con el venv

Con el venv activado:

jupyter lab

Abrir los notebooks en la carpeta notebooks/.

Seleccionar el kernel correspondiente al venv si es necesario.

ğŸš¢ Levantar el proyecto con Docker (Airflow + Jupyter)

1. Clonar el repositorio
   git clone https://github.com/<usuario>/monitor-precios-sipc.git
   cd monitor-precios-sipc

2. Copiar los CSV del SIPC a landing/
   data_sipc/landing/precios_2014.csv
   ...
   data_sipc/landing/productos.csv
   data_sipc/landing/establecimientos.csv

3. Levantar los servicios
   docker compose up -d airflow jupyter

4. Acceder a las interfaces

Airflow: http://localhost:8080

JupyterLab: http://localhost:8888
(la URL con token aparece en los logs del contenedor jupyter-spark).

ğŸŒ€ EjecuciÃ³n del pipeline (Airflow)

Abrir Airflow en http://localhost:8080.

Verificar que el DAG monitor_precios_sipc_dag aparezca en la lista.

Activar el DAG (toggle ON).

Ejecutar una corrida manual (â€œTrigger DAGâ€).

El DAG deberÃ­a:

Leer los CSV desde data_sipc/landing y generar datos limpios en data_sipc/raw.

Construir dimensiones y hechos en data_sipc/refined.

Calcular las mÃ©tricas definidas (promedios, variaciones, min/max, canasta, dispersiÃ³n, ranking).

Exportar datasets listos para el dashboard a data_sipc/exports_dashboard.

ğŸ“Š Notebooks y dashboard

En JupyterLab (http://localhost:8888):

notebooks/01_exploracion.ipynb
ExploraciÃ³n inicial y verificaciÃ³n de calidad de datos.

notebooks/02_modelo_datos.ipynb
ConstrucciÃ³n y validaciÃ³n del modelo de datos (estrella, mÃ©tricas, dimensiones).

notebooks/03_dashboard.ipynb
Dashboard final:

EvoluciÃ³n de precios.

Comparaciones entre supermercados.

Costo de canasta bÃ¡sica.

Ãndice de dispersiÃ³n y ranking de supermercados.

ğŸ‘¥ Equipo

Integrantes: (completar nombres del equipo)

Curso: Big Data â€“ Analista en InformÃ¡tica

Universidad CatÃ³lica del Uruguay â€“ Campus Salto
